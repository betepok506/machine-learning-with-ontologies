{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translational embeddings (TransE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронка учится"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Описание\n",
    "\n",
    "В данном ноутбуке реализуется метод трансляционного встраивания. Он использует векторное представление для отношений, которые имеют те же размеры, что и векторы, представляющие узлы, и определяет операцию перевода как добавление вектора отношения к вектору узла\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы трансляционных эмбеддингов - это семейство методов обучения представлений на графах знаний, которые моделируют отношения в графе знаний как операции перевода между эмбеддингами узлов графа. Эти методы были успешно применены для решения ряда задач, таких как предсказание связей, заполнение­ графов знаний и других. Методы представляют графы знаний как набор ребер (s, p, o) (тройки) и определяют операцию перевода, которая переводит $f_n(s)$ в $f_n(o)$ в зависимости от отношения p. Здесь $f_n$ - вкрапление графа.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Define hyper-parameters inclduing the evaluation information:\n",
    "\n",
    "This is the basic set of parameters for a TransE model\n",
    " \n",
    "Unless you have a powerful GPU or use a different dataset than the one provided, set the number of epochs to a low number (5 or 10) for this tutorial; when you want to use and apply the model for a prediction task later, train for more epochs (ideally, on a good GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_id = '9606' # 4932 - yeast, 9606 - human\n",
    "\n",
    "embeddings_size = 50\n",
    "batch_size = 32\n",
    "margin = 0.1\n",
    "reg_norm = 1\n",
    "optimizer = 'adam'\n",
    "loss_function = 'mse'\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  237/26023 [..............................] - ETA: 10:10 - loss: 362.8941"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9eed272549e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    181\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mbatch_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "class TransE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, nb_classes, nb_relations, embedding_size, margin=0.1, reg_norm=1):\n",
    "        super(TransE, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.nb_relations = nb_relations\n",
    "        self.margin = margin\n",
    "        self.reg_norm = 1\n",
    "        bound = 6 / math.sqrt(embedding_size)\n",
    "        cls_weights = np.random.uniform(low=-bound, high=bound, size=(nb_classes, embedding_size))\n",
    "        rel_weights = np.random.uniform(low=-bound, high=bound, size=(nb_relations, embedding_size))\n",
    "        self.cls_embeddings = tf.keras.layers.Embedding(\n",
    "            nb_classes,\n",
    "            embedding_size,\n",
    "            input_length=1,\n",
    "            weights=[cls_weights,])\n",
    "        self.rel_embeddings = tf.keras.layers.Embedding(\n",
    "            nb_relations,\n",
    "            embedding_size,\n",
    "            input_length=1,\n",
    "            weights=[rel_weights,])\n",
    "    \n",
    "    def reg(self, x):\n",
    "        res = tf.abs(tf.norm(x, axis=1) - self.reg_norm)\n",
    "        res = tf.reshape(res, [-1, 1])\n",
    "        return res\n",
    "    \n",
    "        \n",
    "    def call(self, input):\n",
    "        pos, neg = input\n",
    "        ph, pl, pt = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "        nh, nl, nt = neg[:, 0], neg[:, 1], neg[:, 2]\n",
    "        nh = self.cls_embeddings(nh)\n",
    "        nl = self.rel_embeddings(nl)\n",
    "        nt = self.cls_embeddings(nt)\n",
    "        ph = self.cls_embeddings(ph)\n",
    "        pl = self.rel_embeddings(pl)\n",
    "        pt = self.cls_embeddings(pt)\n",
    "        \n",
    "        pos = tf.subtract(tf.add(ph, pl), pt)\n",
    "        neg = tf.subtract(tf.add(nh, nl), nt)\n",
    "        \n",
    "        self._loss = tf.norm(pos, axis=1) - tf.norm(neg, axis=1) + self.margin\n",
    "        self._loss = tf.reduce_sum(tf.nn.relu(self._loss))\n",
    "        self._loss = (tf.reshape(self._loss, [-1, 1]) + self.reg(ph)\n",
    "                      + self.reg(ph) + self.reg(nh) + self.reg(nh))\n",
    "        return self._loss\n",
    "\n",
    "    \n",
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, data, triple_set, nb_classes, batch_size):\n",
    "        self.data = data\n",
    "        self.triple_set = triple_set\n",
    "        self.nb_classes = nb_classes\n",
    "        self.size = len(data)\n",
    "        self.start = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start < self.size:\n",
    "            end = self.start + self.batch_size\n",
    "            batch = self.data[self.start:end]\n",
    "            neg_triples = []\n",
    "            for h, l, t in batch:\n",
    "                while True:\n",
    "                    hn = h\n",
    "                    tn = t\n",
    "                    if np.random.choice([False, True]):\n",
    "                        hn = np.random.randint(0, self.nb_classes)\n",
    "                    else:\n",
    "                        tn = np.random.randint(0, self.nb_classes)\n",
    "                    if (hn, l, tn) not in self.triple_set:\n",
    "                        neg_triples.append((hn, l, tn))\n",
    "                        break\n",
    "            self.start = end\n",
    "            batch = np.hstack(batch).reshape(len(batch), 3)\n",
    "            neg_triples = np.hstack(neg_triples).reshape(len(neg_triples), 3)\n",
    "            labels = np.zeros((len(batch), 1), dtype='float32')\n",
    "            return ([batch, neg_triples], labels)\n",
    "        else:\n",
    "            self.start = 0\n",
    "            return self.__next__()\n",
    "            #raise StopIteration()\n",
    "\n",
    "\n",
    "def load_data(filename=f'data/train/{org_id}.plain.nt', format='nt'):\n",
    "    g = Graph()\n",
    "    g.parse(filename, format=format)\n",
    "    classes = {}\n",
    "    relations = {}\n",
    "    data = []\n",
    "    for h, l, t in g:\n",
    "        h, l, t = str(h), str(l), str(t)\n",
    "        if h not in classes:\n",
    "            classes[h] = len(classes)\n",
    "        if t not in classes:\n",
    "            classes[t] = len(classes)\n",
    "        if l not in relations:\n",
    "            relations[l] = len(relations)\n",
    "        data.append((classes[h], relations[l], classes[t]))\n",
    "    return data, classes, relations\n",
    "\n",
    "def load_valid_data(data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'http://interacts'\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'http://{it[0]}'\n",
    "            id2 = f'http://{it[1]}'\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((classes[id1], relations[rel], classes[id2]))\n",
    "    return data\n",
    "\n",
    "data, classes, relations = load_data()\n",
    "train_steps = int(math.ceil(len(data) / batch_size))\n",
    "train_generator = Generator(data, set(data), len(classes), batch_size)\n",
    "\n",
    "valid_data = load_valid_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "valid_generator = Generator(valid_data, set(data), len(classes), batch_size)\n",
    "valid_steps = int(math.ceil(len(valid_data) / batch_size))\n",
    "\n",
    "pos_input = Input(shape=(3,), dtype='int32')\n",
    "neg_input = Input(shape=(3,), dtype='int32')\n",
    "transe_model = TransE(len(classes), len(relations), embeddings_size)\n",
    "out = transe_model([pos_input, neg_input])\n",
    "model = tf.keras.Model(inputs=[pos_input, neg_input], outputs=out)\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings:\n",
    "\n",
    "The embeddings provided in the data package have been trained for 100 epochs and may provide better results; you may want to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embeddings = transe_model.cls_embeddings.get_weights()[0]\n",
    "rel_embeddings = transe_model.rel_embeddings.get_weights()[0]\n",
    "cls_embeddings = list(cls_embeddings)\n",
    "rel_embeddings = list(rel_embeddings)\n",
    "\n",
    "if not os.path.exists('data/transe'):\n",
    "    os.makedirs('data/transe')\n",
    "\n",
    "df = pd.DataFrame({'classes': list(classes.keys()), 'embeddings': cls_embeddings})\n",
    "df.to_pickle(f'data/transe/{org_id}_cls_embeddings.pkl')\n",
    "\n",
    "df = pd.DataFrame({'relations': list(relations.keys()), 'embeddings': rel_embeddings})\n",
    "df.to_pickle(f'data/transe/{org_id}_rel_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation results on test set:\n",
    " * Hits@k, k $\\in$ {10, 100}\n",
    " * Mean rank\n",
    " * ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins:  6229\n",
      "0.06 0.32 1125.44 0.82\n",
      "0.13 0.40 1074.69 0.83\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "org_id = '4932'\n",
    "\n",
    "def load_test_data(data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'http://interacts'\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'http://{it[0]}'\n",
    "            id2 = f'http://{it[1]}'\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((id1, rel, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "# Load embeddings from saved files\n",
    "cls_df = pd.read_pickle(f'data/transe/{org_id}_cls_embeddings.pkl')\n",
    "rel_df = pd.read_pickle(f'data/transe/{org_id}_rel_embeddings.pkl')\n",
    "classes = {v: k for k, v in enumerate(cls_df['classes'])}\n",
    "relations = {v: k for k, v in enumerate(rel_df['relations'])}\n",
    "\n",
    "nb_classes = len(cls_df)\n",
    "nb_relations = len(rel_df)\n",
    "embeds_list = cls_df['embeddings'].values\n",
    "rembeds_list = rel_df['embeddings'].values\n",
    "size = len(embeds_list[0])\n",
    "embeds = np.zeros((nb_classes, size), dtype=np.float32)\n",
    "for i, emb in enumerate(embeds_list):\n",
    "    embeds[i, :] = emb\n",
    "\n",
    "proteins = {}\n",
    "for k, v in classes.items():\n",
    "    if not k.startswith('http://GO:'):\n",
    "        proteins[k] = v\n",
    "\n",
    "print('Proteins: ', len(proteins))\n",
    "\n",
    "prot_index = list(proteins.values())\n",
    "prot_embeds = embeds[prot_index, :]\n",
    "prot_dict = {v: k for k, v in enumerate(prot_index)}\n",
    "    \n",
    "rsize = len(rembeds_list[0])\n",
    "rembeds = np.zeros((nb_relations, rsize), dtype=np.float32)\n",
    "for i, emb in enumerate(rembeds_list):\n",
    "    rembeds[i, :] = emb\n",
    "\n",
    "# Load training data to computed filtered rank\n",
    "train_data = load_test_data(f'data/train/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "trlabels = {}\n",
    "for c, r, d in train_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "for c, r, d in valid_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "\n",
    "\n",
    "# Load test data and compute ranks for each protein\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, r, d in eval_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in labels:\n",
    "        labels[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    if r not in preds:\n",
    "        preds[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.float32)\n",
    "    labels[r][c, d] = 1\n",
    "    ec = prot_embeds[c, :]\n",
    "    er = rembeds[r, :]\n",
    "    ec += er\n",
    "\n",
    "    # Compute distance\n",
    "    dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    res = dst.flatten()\n",
    "\n",
    "    preds[r][c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[r][c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
