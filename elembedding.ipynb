{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тут все работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Описание\n",
    "\n",
    "В данном ноутбуке используется семантический подход. [Ссылка на их же статьи про эту модель](https://arxiv-org.translate.goog/abs/1902.10499?_x_tr_sl=auto&_x_tr_tl=ru&_x_tr_hl=ru)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import re\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import rankdata\n",
    "import os\n",
    "\n",
    "from elembeddings.elembedding import (\n",
    "    ELModel, load_data, load_valid_data, Generator, MyModelCheckpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 256\n",
    "embedding_size = 50\n",
    "margin = -0.1\n",
    "reg_norm = 1\n",
    "learning_rate = 1e-3\n",
    "epochs = 1\n",
    "org_id = '9606'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf1 85119\n",
      "nf2 12090\n",
      "nf3 749275\n",
      "nf4 12088\n",
      "disjoint 30\n",
      "nf3_neg 1076572\n",
      "top 1\n"
     ]
    }
   ],
   "source": [
    "# Load training data in (h, l, t) triples\n",
    "# classes and relations are entity to id mappings\n",
    "train_data, classes, relations = load_data(f'data/train/{org_id}.classes-normalized.owl')\n",
    "valid_data = load_valid_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "for key, value in train_data.items():\n",
    "    print(f'{key} {len(value)}')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes 61810\n",
      "Total number of relations 10\n"
     ]
    }
   ],
   "source": [
    "# Filter out protein classes\n",
    "proteins = {}\n",
    "for k, v in classes.items():\n",
    "    if not k.startswith('<http://purl.obolibrary.org/obo/GO_'):\n",
    "        proteins[k] = v\n",
    "\n",
    "# Prepare data for training the model\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "nb_train_data = 0\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "# id to entity maps\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "        \n",
    "print('Total number of classes', nb_classes)\n",
    "print('Total number of relations', nb_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ELEmbeddings Model and Train\n",
    "\n",
    "Embeddings are saved depending on mean rank evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4203/4206 [============================>.] - ETA: 0s - loss: 14.2465\n",
      " Validation 1 4298.5386063254955\n",
      "\n",
      "\n",
      " Saving embeddings 1 4298.5386063254955\n",
      "\n",
      "4206/4206 [==============================] - 451s 107ms/step - loss: 14.2384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f663a2acd30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input layers for each loss type\n",
    "nf1 = Input(shape=(2,), dtype=np.int32)\n",
    "nf2 = Input(shape=(3,), dtype=np.int32)\n",
    "nf3 = Input(shape=(3,), dtype=np.int32)\n",
    "nf4 = Input(shape=(3,), dtype=np.int32)\n",
    "dis = Input(shape=(3,), dtype=np.int32)\n",
    "top = Input(shape=(1,), dtype=np.int32)\n",
    "nf3_neg = Input(shape=(3,), dtype=np.int32)\n",
    "\n",
    "# Build model\n",
    "el_model = ELModel(nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm)\n",
    "out = el_model([nf1, nf2, nf3, nf4, dis, top, nf3_neg])\n",
    "model = tf.keras.Model(inputs=[nf1, nf2, nf3, nf4, dis, top, nf3_neg], outputs=out)\n",
    "optimizer = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "if not os.path.exists('data/elembeddings'):\n",
    "    os.makedirs('data/elembeddings')\n",
    "    \n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "# ModelCheckpoint which runs at the end of each epoch\n",
    "checkpointer = MyModelCheckpoint(\n",
    "    out_classes_file=out_classes_file,\n",
    "    out_relations_file=out_relations_file,\n",
    "    cls_list=cls_list,\n",
    "    rel_list=rel_list,\n",
    "    valid_data=valid_data,\n",
    "    proteins=proteins,\n",
    "    monitor='loss')\n",
    "\n",
    "# Start training\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    workers=12,\n",
    "    callbacks=[checkpointer,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of embeddings on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 9606\n",
      "0.01 0.10 4160.10 0.75\n",
      "0.03 0.20 4084.90 0.75\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'<http://interacts>'\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'<http://{it[0]}>'\n",
    "            id2 = f'<http://{it[1]}>'\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((id1, rel, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "cls_df = pd.read_pickle(out_classes_file)\n",
    "rel_df = pd.read_pickle(out_relations_file)\n",
    "nb_classes = len(cls_df)\n",
    "nb_relations = len(rel_df)\n",
    "embeds_list = cls_df['embeddings'].values\n",
    "rembeds_list = rel_df['embeddings'].values\n",
    "size = len(embeds_list[0])\n",
    "embeds = np.zeros((nb_classes, size), dtype=np.float32)\n",
    "for i, emb in enumerate(embeds_list):\n",
    "    embeds[i, :] = emb\n",
    "\n",
    "rs = np.abs(embeds[:, -1]).reshape(-1, 1)\n",
    "embeds = embeds[:, :-1]\n",
    "prot_index = list(proteins.values())\n",
    "prot_rs = rs[prot_index, :]\n",
    "prot_embeds = embeds[prot_index, :]\n",
    "prot_dict = {v: k for k, v in enumerate(prot_index)}\n",
    "    \n",
    "rsize = len(rembeds_list[0])\n",
    "rembeds = np.zeros((nb_relations, rsize), dtype=np.float32)\n",
    "for i, emb in enumerate(rembeds_list):\n",
    "    rembeds[i, :] = emb\n",
    "\n",
    "train_data = load_test_data(f'data/train/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "trlabels = {}\n",
    "for c, r, d in train_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "for c, r, d in valid_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, r, d in eval_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in labels:\n",
    "        labels[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    if r not in preds:\n",
    "        preds[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.float32)\n",
    "    labels[r][c, d] = 1\n",
    "    ec = prot_embeds[c, :]\n",
    "    rc = prot_rs[c, :]\n",
    "    er = rembeds[r, :]\n",
    "    ec += er\n",
    "\n",
    "    # Compute similarity\n",
    "    dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    dst = dst.reshape(-1, 1)\n",
    "    res = np.maximum(0, dst - rc - prot_rs - margin)\n",
    "    res = res.flatten()\n",
    "\n",
    "    preds[r][c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[r][c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'Evaluation for {org_id}')\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
