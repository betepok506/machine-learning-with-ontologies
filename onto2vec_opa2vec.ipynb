{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we run two ontology based methods to produce vector representations of biological entities: Onto2Vec and OPA2Vec.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не работает пункт `Cosine similarity`, требует передачи каких-то параметров. Также при получении векторов Word2Vec никак не обрабатываются отсутствующие слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Описание\n",
    "\n",
    "В данном ноутбуке Onto2vec создает векторные представления на основе логических аксиом онтологии и известных ассоциаций между классами онтологий и биологическими объектами. В приведенном ниже примере мы используем Onto2vec для создания векторных представлений белков на основе их аннотаций GO и логических аксиом GO.\n",
    "\n",
    "\n",
    "В дополнение к аксиомам онтологии и их ассоциациям сущностей, OPA2Vec также использует метаданные онтологии и литературу для представления биологических сущностей.\n",
    "\n",
    "Далее вычисляется сходства косинусов для изучения соседей каждого белка и поиска наиболее похожих белковых векторов. Затем выполняется предсказание взаимодействия на основе значения сходства, основанного на предположении, что белки с очень похожими векторами признаков больше склонны к взаимодействию\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto2vec produces vectory representations based on the logical axioms of an ontology and the known associations between ontology classes and biological entities. In the case study below, we use Onto2vec to produce vector representations of proteins based on their GO annotations and the GO logical axioms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае ошибки `Can't locate List/MoreUtils.pm in @INC ` выполнить команду `sudo apt install liblist-moreutils-perl`[ссылка на проблему](https://www.reddit.com/r/bioinformatics/comments/t8scex/error_message_cant_locate_listmoreutilspm_in_inc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_id ='9606' #or 9606 for human data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\t\t*********** Onto2Vec Running ... ***********\n",
      "\n",
      "\n",
      "\t\t1.Reasoning over ontology ...\n",
      "\n",
      "Команда запуска: groovy opa2vec/ProcessOntology.groovy data/go.owl elk /tmp/tmp4gow5c4d /tmp/tmpkobhcena /tmp/tmpdu8tutxp\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.codehaus.groovy.reflection.CachedClass (file:/usr/share/groovy/lib/groovy-2.4.21.jar) to method java.lang.Object.finalize()\n",
      "WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.reflection.CachedClass\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Loading of Axioms ...\n",
      "Loading ...\n",
      "    1%\n",
      "    2%\n",
      "    3%\n",
      "    4%\n",
      "    6%\n",
      "    7%\n",
      "    8%\n",
      "    10%\n",
      "    12%\n",
      "    13%\n",
      "    15%\n",
      "    17%\n",
      "    19%\n",
      "    22%\n",
      "    23%\n",
      "    25%\n",
      "    27%\n",
      "    30%\n",
      "    32%\n",
      "    35%\n",
      "    36%\n",
      "    38%\n",
      "    40%\n",
      "    43%\n",
      "    46%\n",
      "    49%\n",
      "    52%\n",
      "    54%\n",
      "    57%\n",
      "    59%\n",
      "    62%\n",
      "    65%\n",
      "    67%\n",
      "    70%\n",
      "    72%\n",
      "    75%\n",
      "    77%\n",
      "    80%\n",
      "    84%\n",
      "    87%\n",
      "    90%\n",
      "    94%\n",
      "    97%\n",
      "    ... finished\n",
      "    ... finished\n",
      "Property Saturation Initialization ...\n",
      "    ... finished\n",
      "Reflexive Property Computation ...\n",
      "    ... finished\n",
      "Object Property Hierarchy and Composition Computation ...\n",
      "    ... finished\n",
      "Context Initialization ...\n",
      "    ... finished\n",
      "Consistency Checking ...\n",
      "    100%\n",
      "    ... finished\n",
      "Class Taxonomy Computation ...\n",
      "    34%\n",
      "    75%\n",
      "    98%\n",
      "    99%\n",
      "    ... finished\n",
      "Команда запуска: cat /tmp/tmpkobhcena /tmp/tmp4gow5c4d >/tmp/tmp07ja8ooj\n",
      "Команда запуска: perl opa2vec/getclasses.pl data/train/9606.OPA_associations.txt /tmp/tmpc17774mi\n",
      "Команда запуска: cat /tmp/tmpdu8tutxp /tmp/tmpc17774mi > /tmp/tmp3f4gdqx3\n",
      "Команда запуска: sort -u /tmp/tmp3f4gdqx3 > /tmp/tmp1wpt4l4l\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t2.Propagate associations through hierarchy ...\n",
      "\n",
      "Команда запуска: perl opa2vec/AddAncestors.pl data/train/9606.OPA_associations.txt /tmp/tmp07ja8ooj /tmp/tmpdu8tutxp /tmp/tmpx6dh50zg\n",
      "Команда запуска: cat data/train/9606.OPA_associations.txt /tmp/tmpx6dh50zg > /tmp/tmp5k615vyo\n",
      "Команда запуска: sort -u /tmp/tmp5k615vyo > /tmp/tmpnebnnn5g\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t3.Corpus creation ...\n",
      "\n",
      "Команда запуска: cat /tmp/tmp07ja8ooj /tmp/tmpnebnnn5g > /tmp/tmpeyvad_35\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t4.Running word2Vec ... \n",
      "\n",
      "Команда запуска: python onto2vec/runWV_onto.py /tmp/tmpeyvad_35 5 200 0 sg data/train/9606.protein.links.v11.0.txt /tmp/tmpcxtjuo4p\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t5.Processing vectors ... \n",
      "\n",
      "Команда запуска: python opa2vec/process_vectors.py /tmp/tmpcxtjuo4p > data/9606.onto2vec_vecs\n"
     ]
    }
   ],
   "source": [
    "!python onto2vec/runOnto2Vec.py  -ontology data/go.owl -associations data/train/{org_id}.OPA_associations.txt -outfile data/{org_id}.onto2vec_vecs -entities data/train/{org_id}.protein.links.v11.0.txt  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OPA2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the ontology axioms and their entity associations, OPA2Vec also uses the ontology metadata and literature to represent biological entities. The code below runs OPA2Vec on GO and protein-GO associations to produce protein vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перед запуском необходимо скачать файлы `RepresentationModel_pubmed.txt`,`RepresentationModel_pubmed.txt.syn1neg.npy`,`RepresentationModel_pubmed.txt.wv.syn0.npy` модели Word2Vec и поместить в каталог `opa2vec`. [Ссылка для скачивания](https://bio2vec.cbrc.kaust.edu.sa/data/pubmed_model/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\t\t*********** OPA2Vec Running ... ***********\n",
      "\n",
      "\n",
      "\t\t1.Ontology Processing ...\n",
      "\n",
      "Команда запсука: groovy opa2vec/ProcessOntology.groovy data/go.owl elk /tmp/tmpebgph7ne /tmp/tmpglvykdr6 /tmp/tmp_0wsawli\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.codehaus.groovy.reflection.CachedClass (file:/usr/share/groovy/lib/groovy-2.4.21.jar) to method java.lang.Object.finalize()\n",
      "WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.reflection.CachedClass\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Loading of Axioms ...\n",
      "Loading ...\n",
      "    1%\n",
      "    2%\n",
      "    3%\n",
      "    4%\n",
      "    5%\n",
      "    7%\n",
      "    9%\n",
      "    11%\n",
      "    12%\n",
      "    14%\n",
      "    16%\n",
      "    18%\n",
      "    20%\n",
      "    22%\n",
      "    24%\n",
      "    26%\n",
      "    28%\n",
      "    29%\n",
      "    31%\n",
      "    33%\n",
      "    35%\n",
      "    36%\n",
      "    38%\n",
      "    40%\n",
      "    42%\n",
      "    44%\n",
      "    46%\n",
      "    48%\n",
      "    50%\n",
      "    52%\n",
      "    54%\n",
      "    56%\n",
      "    58%\n",
      "    61%\n",
      "    63%\n",
      "    65%\n",
      "    67%\n",
      "    69%\n",
      "    72%\n",
      "    74%\n",
      "    77%\n",
      "    79%\n",
      "    82%\n",
      "    84%\n",
      "    86%\n",
      "    88%\n",
      "    91%\n",
      "    93%\n",
      "    95%\n",
      "    97%\n",
      "    ... finished\n",
      "    ... finished\n",
      "Property Saturation Initialization ...\n",
      "    ... finished\n",
      "Reflexive Property Computation ...\n",
      "    ... finished\n",
      "Object Property Hierarchy and Composition Computation ...\n",
      "    ... finished\n",
      "Context Initialization ...\n",
      "    ... finished\n",
      "Consistency Checking ...\n",
      "    100%\n",
      "    ... finished\n",
      "Class Taxonomy Computation ...\n",
      "    26%\n",
      "    47%\n",
      "    74%\n",
      "    98%\n",
      "    99%\n",
      "    ... finished\n",
      "Команда запсука: cat /tmp/tmpglvykdr6 /tmp/tmpebgph7ne >/tmp/tmp6qsvn1qg\n",
      "Команда запсука: perl opa2vec/getclasses.pl data/train/9606.OPA_associations.txt /tmp/tmpxkvi76d1\n",
      "Команда запсука: cat /tmp/tmp_0wsawli /tmp/tmpxkvi76d1 > /tmp/tmppoiltm41\n",
      "Команда запсука: sort -u /tmp/tmppoiltm41 > /tmp/tmpejchzuf0\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t2.Metadata Extraction ...\n",
      "\n",
      "Команда запсука: groovy opa2vec/getMetadata.groovy data/go.owl all /tmp/tmp4xeb4ro6\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.codehaus.groovy.reflection.CachedClass (file:/usr/share/groovy/lib/groovy-2.4.21.jar) to method java.lang.Object.finalize()\n",
      "WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.reflection.CachedClass\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t3.Propagate Associations through hierarchy ...\n",
      "\n",
      "Команда запсука: sort -u /tmp/tmpou4fonpg > /tmp/tmpr3pp172v\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t4.Corpus Creation ...\n",
      "\n",
      "Команда запсука: cat /tmp/tmp6qsvn1qg /tmp/tmp4xeb4ro6 /tmp/tmpr3pp172v > /tmp/tmpnxf9w69b\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t5.Running Word2Vec ... \n",
      "\n",
      "Команда запсука: python opa2vec/runWord2Vec.py /tmp/tmpejchzuf0 5 200 0 sg opa2vec/RepresentationModel_pubmed.txt /tmp/tmpnxf9w69b /tmp/tmpypcx3cbb\n",
      "/tmp/tmpypcx3cbb\n",
      "/tmp/tmpejchzuf0\n",
      "\t\t*********** Vector representations created ***********\n",
      "\n",
      "\n",
      "\t\t*********** OPA2Vec Complete ***********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python opa2vec/runOPA2Vec.py  -ontology data/go.owl -associations data/train/{org_id}.OPA_associations.txt -outfile data/{org_id}.opa2vec_vecs -entities data/train/{org_id}.protein.links.v11.0.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map proteins to corresponding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_id = '9606' #org_id = '4932'\n",
    "onto2vec_map = {} \n",
    "opa2vec_map = {}\n",
    "with open (f'data/{org_id}.onto2vec_vecs','r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "            onto2vec_map[protein]=vector\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "with open (f'data/{org_id}.opa2vec_vecs','r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "            opa2vec_map[protein]=vector\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pair features for the training/validation/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "data_type = ['train', 'valid', 'test']\n",
    "for i in data_type:\n",
    "    pair_data = []\n",
    "    feature_vecs =[]\n",
    "    label_map ={}\n",
    "    with open (f'data/{i}/{org_id}.protein.links.v11.0.txt','r') as f1:\n",
    "        for line in f1:\n",
    "            prot1, prot2 = line.strip().split()\n",
    "            pair_data.append((prot1,prot2))\n",
    "            label_map[(prot1, prot2)] = 1\n",
    "            \n",
    "    with open (f'data/{i}/{org_id}.negative_interactions.txt','r') as f2:\n",
    "        for line in f2:\n",
    "            prot1, prot2 = line.strip().split()\n",
    "            pair_data.append((prot1, prot2))\n",
    "            label_map[(prot1, prot2)] = 0 \n",
    "            \n",
    "    random.shuffle(pair_data)\n",
    "    with open (f'data/{i}/{org_id}.onto2vec_features','w') as f3:\n",
    "        with open (f'data/{i}/{org_id}.opa2vec_features', 'w') as f4:\n",
    "            with open (f'data/{i}/{org_id}.labels','w') as f5:\n",
    "                with open (f'data/{i}/{org_id}.pairs','w') as f6:\n",
    "                    for prot1, prot2 in pair_data:\n",
    "                        if (prot1 in onto2vec_map and prot1 in opa2vec_map and prot2 in onto2vec_map and prot2 in opa2vec_map):\n",
    "                            f6.write (f'{prot1} {prot2}\\n')\n",
    "                            f5.write (f'{label_map[(prot1,prot2)]}\\n')\n",
    "                            f4.write (f'{opa2vec_map[prot1]} {opa2vec_map[prot2]}\\n')\n",
    "                            f3.write (f'{onto2vec_map[prot1]} {onto2vec_map[prot2]}\\n')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculating cosine similarity to explore neighbors of each protein and finding most similar protein vectors. The interaction prediction is then performed based on similarity value based on the assumption that proteins with highly similar feature vectors are more like to interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/home/andrey/.local/share/jupyter/runtime/kernel-810eab24-08c1-4253-a389-36a58e585564.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-653570d115a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#query =\"A0A024RBG1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#n=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/home/andrey/.local/share/jupyter/runtime/kernel-810eab24-08c1-4253-a389-36a58e585564.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine \n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "for prot1, prot2 in pair_data:\n",
    "    try:\n",
    "        v1_onto = onto2vec_map[prot1]\n",
    "        v2_onto = onto2vec_map [prot2]\n",
    "        v1_opa = opa2vec_map [prot1]\n",
    "        v2_opa = opa2vec_map [prot2]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    if (prot1 in onto2vec_map and prot1 in opa2vec_map and prot2 in onto2vec_map and prot2 in opa2vec_map):\n",
    "        cosine_onto = cosine(v1_onto, v2_onto)\n",
    "        cosine_opa = cosine (v1_opa, v2_opa)\n",
    "        \n",
    "    with open (f'data/{i}/{org_id}.onto_sim','w') as onto_cos:\n",
    "        with open (f'data/{i}/{org_id}.opa_sim','w') as opa_cos:\n",
    "            onto_cos.write (f'{cosine_onto}\\n')\n",
    "            opa_cos.write (f'{cosine_onto}\\n')\n",
    "        \n",
    "        \n",
    "query =str(sys.argv[1])\n",
    "n = int (sys.argv[2])\n",
    "#query =\"A0A024RBG1\"\n",
    "#n=10\n",
    "vectors=numpy.loadtxt(\"data/{org_id}.opa2vec_vecs\");\n",
    "text_file=\"data/train/protein_list\"\n",
    "classfile=open (text_file)\n",
    "mylist=[]\n",
    "for linec in classfile:\n",
    "\tmystr=linec.strip()\n",
    "\tmylist.append(mystr)\n",
    "\n",
    "\n",
    "#3.Mapping Entities to Vectors\n",
    "vectors_map={}\n",
    "for i in range(0,len(mylist)):\n",
    "\tvectors_map[mylist[i]]=vectors[i,:]\n",
    "\t\n",
    "\n",
    "\n",
    "cosine_sim={}\n",
    "for x in range(0,len(mylist)):\n",
    "\tif (mylist[x]!=query): \t\n",
    "\t\tv1=vectors_map[mylist[x]]\n",
    "\t\tv2=vectors_map[query]\n",
    "\t\tvalue=cosine(v1,v2)\n",
    "\t\tcosine_sim[mylist[x]]=value\n",
    "        \n",
    "classes = mylist\n",
    "#5.Retrieving neighbors \n",
    "sortedmap=sorted(cosine_sim,key=cosine_sim.get, reverse=True)\n",
    "iterator=islice(sortedmap,n)\n",
    "i =1\n",
    "for d in iterator:\n",
    "\tprint (str(i)+\". \"+ str(d) +\"\\t\"+str(cosine_sim[d])+\"\\n\")\n",
    "\ti +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_test_data(data_file, classes):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'http://{it[0]}'\n",
    "            id2 = f'http://{it[1]}'\n",
    "            data.append((id1, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "\n",
    "# Load test data and compute ranks for each protein\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, d in eval_data:\n",
    "    c, d = prot_dict[classes[c]], prot_dict[classes[d]]\n",
    "    labels = np.zeros((len(onto2vec_map), len(onto2vec_map)), dtype=np.int32)\n",
    "    preds = np.zeros((len(onto2vec_map), len(onto2vec_map)), dtype=np.float32)\n",
    "    labels[c, d] = 1\n",
    "    ec = onto2vec_map[c, :]\n",
    "    #er = rembeds[r, :]\n",
    "    #ec += er\n",
    "\n",
    "    # Compute distance\n",
    "    #dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    res = numpy.loadtxt('onto_cos.write')\n",
    "\n",
    "    preds[c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from random import randint \n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import sys \n",
    "import numpy\n",
    "import sklearn \n",
    "\n",
    "#Hyperparameters\n",
    "num_epochs = 100\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "#Load dataset \n",
    "X_train_1= numpy.loadtxt(\"data/train/{org_id}.embeddings_1\")\n",
    "X_train_2= numpy.loadtxt(\"data/train/{org_id}.embeddings_2\")\n",
    "y_train= numpy.loadtxt(\"data/train/{org_id}.labels\")\n",
    "\n",
    "X_test_1= numpy.loadtxt(\"data/test/{org_id}.embeddings_1\")\n",
    "X_test_2= numpy.loadtxt(\"data/test/{org_id}.embeddings_2\")\n",
    "y_test= numpy.loadtxt(\"data/test/{org_id}.labels\")\n",
    "\n",
    "#transform to torch\n",
    "train_x1= torch.from_numpy(X_train_1).float()\n",
    "train_x2= torch.from_numpy(X_train_2).float()\n",
    "train_x = [train_x1, train_x2]\n",
    "train_label= torch.from_numpy(y_train).long()\n",
    "\n",
    "\n",
    "test_x1 = torch.from_numpy(X_test_1).float()\n",
    "test_x2 = torch.from_numpy(X_test_2).float()\n",
    "test_x=[test_x1, test_x2]\n",
    "test_label= torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_data.append([train_x, train_label])\n",
    "\n",
    "test_data = []\n",
    "test_data.append([test_x,test_label])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Define Network \n",
    "class Net (nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\tnn.Linear (200, 600),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer2 = nn.Sequential (\n",
    "\t\t\tnn.Linear (600,400),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\tnn.Linear (400, 200),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.drop_out = nn.Dropout()\n",
    "\t\tself.dis = nn.Linear (200,2)\n",
    "\n",
    "\t\t\t\t\n",
    "\tdef forward (self, data):\n",
    "\t\tres = []\n",
    "\t\tfor i in range(2):\n",
    "\t\t\tx = data[i]\n",
    "\t\t\tout = self.layer1(x)\n",
    "\t\t\tout = self.layer2(out)\n",
    "\t\t\tout = self.layer3(out)\n",
    "\t\t\tout = self.drop_out(out)\n",
    "\t\t\t#out = out.reshape(out.size(0),-1)\n",
    "\t\t\tres.append(out)\n",
    "\t\toutput = torch.abs(res[1] - res[0])\n",
    "\t\t#output = torch.mm(res[1] , res[0])\t\t\n",
    "\t\toutput = self.dis(output)\n",
    "\t\treturn output\n",
    "\n",
    "#Create network \n",
    "network = Net()\n",
    "\n",
    "# Use Cross Entropy for back propagation \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam (network.parameters(),lr=learning_rate)\n",
    "\n",
    "# Train the model \n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range (num_epochs):\n",
    "\tfor i, (train_x, train_label) in enumerate (train_loader):\n",
    "\t\t# Get data\n",
    "\t\tinputs = train_x\n",
    "\t\tlabels = train_label\n",
    "\n",
    "\t\t# Run the forward pass\n",
    "\t\toutputs = network (inputs)\n",
    "\t\toutputs=outputs.reshape(-1,2)\n",
    "\t\tlabels=labels.reshape(-1)\t\t\t\t\n",
    "\t\t#print (outputs.size())\n",
    "\t\t#print (labels.size())\n",
    "\t\tloss = criterion (outputs, labels)\n",
    "\t\tloss_list.append(loss.item())\n",
    "\t\n",
    "\t\t# Back propagation and optimization\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Get prediction\n",
    "\t\ttotal = labels.size(0)\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\tcorrect = (predicted == labels).sum().item()\n",
    "\t\tacc_list.append (correct/total)\n",
    "\t\t\n",
    "\t\t#if (i + 1) % 100 == 0:\n",
    "\t\tprint ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct/total)*100))\n",
    "\n",
    "\n",
    "# Test the model \n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\tfor test_x,test_label in  test_loader:\n",
    "\t\toutputs = network (test_x)\n",
    "\t\tlabels = test_label\n",
    "\t\toutputs=outputs.reshape(-1,2)\t\t\n",
    "\t\tarray = outputs.data.cpu().numpy()\n",
    "\t\tnumpy.savetxt('output.csv',array)\n",
    "\t\tlabels=labels.reshape(-1)\t\n",
    "\t\t_, predicted = torch.max(outputs.data,1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t#print ('Accuracy of model on test dataset is: {} %'.format((correct / total) *100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
