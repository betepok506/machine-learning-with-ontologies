{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Neural Network for predicting PPIs from function annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронка учится"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Описание\n",
    "\n",
    "\n",
    "Данный блокнот содержит реализация сиамской сети, которая предсказывает, учитывая два эмбеддинга в качестве входных данных, должны ли белки, которые они представляют, взаимодействовать. На вход, судя по всему, подается вектор взаимодействия белков (one-hot).\n",
    "\n",
    "Вырезка из статьи\n",
    "```\n",
    "Машинное обучение также может быть использовано для аппроксимации ­функций, которые принимают на вход более одного встраивания, и эти функции затем могут быть использованы для предсказания отношений между сущностями, которые были вложены [34], или для изучения мер сходства между сущностями онтологии. Например, если сходство между двумя эмбеддингими белков должно быть мерой того, взаимодействуют они или нет, использование набора белков, которые взаимодействуют, может быть использовано для обучения функции, которая предсказывает, учитывая два эмбеддинги в качестве входных данных, должны ли белки, которые они представляют, взаимодействовать. Многие архитектуры нейронных сетей и другие модели машинного обучения могут быть использованы для этой задачи; архитектуры, которые используются для обучения по сходству, такие как сиамские нейронные сети, похоже, хорошо работают на практике [63].\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/andrey/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import click as ck\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Concatenate, Dot, Activation\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "from elembeddings.utils import Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins in training:  16371\n",
      "Training interactions:  538286\n",
      "Validation interactions:  133618\n",
      "Testing interactions:  167068\n"
     ]
    }
   ],
   "source": [
    "org_id = '9606'\n",
    "\n",
    "def load_train_data(data_file):\n",
    "    data = []\n",
    "    proteins = {}\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins:\n",
    "                proteins[id1] = len(proteins)\n",
    "            if id2 not in proteins:\n",
    "                proteins[id2] = len(proteins)\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data, proteins\n",
    "\n",
    "def load_test_data(data_file, proteins):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins or id2 not in proteins:\n",
    "                continue\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data\n",
    "\n",
    "train_data, proteins = load_train_data(f'data/train/{org_id}.protein.links.v11.0.txt')\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "print('Number of proteins in training: ', len(proteins))\n",
    "print('Training interactions: ', len(train_data))\n",
    "print('Validation interactions: ', len(valid_data))\n",
    "print('Testing interactions: ', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functional annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 14888 proteins\n",
      "Total number of distinct functions 15494\n"
     ]
    }
   ],
   "source": [
    "def load_annotations(data_file, proteins, propagate=False):\n",
    "    go = Ontology('data/go.obo')\n",
    "    annots = {}\n",
    "    functions = set()\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split('\\t')\n",
    "            if it[0] not in proteins:\n",
    "                continue\n",
    "            p_id = proteins[it[0]]\n",
    "            if p_id not in annots:\n",
    "                annots[p_id] = set()\n",
    "            annots[p_id].add(it[1])\n",
    "            functions.add(it[1])\n",
    "            if propagate and go.has_term(it[1]):\n",
    "                annots[p_id] |= go.get_anchestors(it[1])\n",
    "                functions |= go.get_anchestors(it[1])\n",
    "    functions = list(functions)\n",
    "    return annots, functions\n",
    "\n",
    "# Run this function with propagate=False to use annotations without propagation with ontology structure\n",
    "annotations, functions = load_annotations(f'data/train/{org_id}.annotation.txt', proteins, propagate=False)\n",
    "print('Loaded annotations for', len(annotations), 'proteins')\n",
    "print('Total number of distinct functions', len(functions))\n",
    "functions_ix = {k:i for i, k in enumerate(functions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator object for feeding neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, data, proteins, annotations, train_pairs, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.train_pairs = train_pairs\n",
    "        self.proteins = proteins\n",
    "        self.annotations = annotations\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pos = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            batch_neg = []\n",
    "            for pr1, pr2 in batch_pos:\n",
    "                flag = np.random.choice([True, False])\n",
    "                while True:\n",
    "                    neg = np.random.randint(0, len(self.proteins))\n",
    "                    if flag:\n",
    "                        if (pr1, neg) not in train_pairs:\n",
    "                            batch_neg.append((pr1, neg))\n",
    "                            break\n",
    "                    else:\n",
    "                        if (neg, pr2) not in train_pairs:\n",
    "                            batch_neg.append((neg, pr2))\n",
    "                            break\n",
    "            batch_data = np.array(batch_pos + batch_neg)\n",
    "            labels = np.array([1] * len(batch_pos) + [0] * len(batch_neg))\n",
    "            index = np.arange(len(batch_data))\n",
    "            np.random.shuffle(index)\n",
    "            batch_data = batch_data[index]\n",
    "            labels = labels[index]\n",
    "            p1 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_data)):\n",
    "                if batch_data[i, 0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_data[i, 1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "train_pairs = set(train_data)\n",
    "batch_size = 128\n",
    "train_steps = int(math.ceil(len(train_data) / batch_size))\n",
    "train_generator = Generator(\n",
    "    train_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=train_steps)\n",
    "valid_steps = int(math.ceil(len(valid_data) / batch_size))\n",
    "valid_generator = Generator(\n",
    "    valid_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=valid_steps)\n",
    "test_steps = int(math.ceil(len(test_data) / batch_size))\n",
    "test_generator = Generator(\n",
    "    test_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15494)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15494)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          16523008    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 16,523,008\n",
      "Trainable params: 16,523,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_model = Sequential()\n",
    "feature_model.add(Dense(1024, input_shape=(len(functions),), activation='relu'))\n",
    "feature_model.add(Dense(512, activation='relu'))\n",
    "feature_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "input1 = Input(shape=(len(functions),))\n",
    "input2 = Input(shape=(len(functions),))\n",
    "feature1 = feature_model(input1)\n",
    "feature2 = feature_model(input2)\n",
    "net = Dot(axes=1)([feature1, feature2])\n",
    "net = Activation('sigmoid')(net)\n",
    "model = Model(inputs=[input1, input2], outputs=net)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "  16/4206 [..............................] - ETA: 28:12 - loss: 0.6602"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0e7ba55ae3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     callbacks=[earlystopper,])\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/problem/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "earlystopper = EarlyStopping(patience=3)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=[earlystopper,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306/1306 [==============================] - 27s 20ms/step\n",
      "Test loss: 0.4613165470497025\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate_generator(test_generator, steps=test_steps, verbose=1)\n",
    "print('Test loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction scores for all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test proteins: 16371\n",
      "2093826/2093826 [==============================] - 21119s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Total number of test proteins:', len(proteins))\n",
    "all_pairs = []\n",
    "for i in range(len(proteins)):\n",
    "    for j in range(len(proteins)):\n",
    "        all_pairs.append((i, j))\n",
    "\n",
    "batch_size = 128\n",
    "class SimpleGenerator(object):\n",
    "\n",
    "    def __init__(self, data, annotations, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.annotations = annotations\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pairs = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            p1 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_pairs)):\n",
    "                if batch_pairs[i][0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_pairs[i][1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            labels = np.zeros((len(batch_pairs), 1), dtype=np.float32)\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "all_steps = int(math.ceil(len(all_pairs) / batch_size))\n",
    "all_generator = SimpleGenerator(\n",
    "    all_pairs, annotations, functions_ix,\n",
    "    batch_size=batch_size, steps=all_steps)\n",
    "predictions = model.predict_generator(all_generator, steps=all_steps, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.05 0.41 1881.10 0.90\n",
      "0.15 0.64 1808.77 0.89\n"
     ]
    }
   ],
   "source": [
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "sim = predictions.reshape(len(proteins), len(proteins))\n",
    "\n",
    "trlabels = np.ones((len(proteins), len(proteins)), dtype=np.int32)\n",
    "for c, d in train_data:\n",
    "    trlabels[c, d] = 0\n",
    "for c, d in valid_data:\n",
    "    trlabels[c, d] = 0\n",
    "\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "n = len(test_data)\n",
    "labels = np.zeros((len(proteins), len(proteins)), dtype=np.int32) \n",
    "ranks = {}\n",
    "franks = {}\n",
    "with ck.progressbar(test_data) as prog_data:\n",
    "    for c, d in prog_data:\n",
    "        labels[c, d] = 1\n",
    "        index = rankdata(-sim[c, :], method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            top10 += 1\n",
    "        if rank <= 100:\n",
    "            top100 += 1\n",
    "        mean_rank += rank\n",
    "        if rank not in ranks:\n",
    "            ranks[rank] = 0\n",
    "        ranks[rank] += 1\n",
    "\n",
    "        # Filtered rank\n",
    "        fil = sim[c, :] * (labels[c, :] | trlabels[c, :])\n",
    "        index = rankdata(-fil, method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            ftop10 += 1\n",
    "        if rank <= 100:\n",
    "            ftop100 += 1\n",
    "        fmean_rank += rank\n",
    "        if rank not in franks:\n",
    "            franks[rank] = 0\n",
    "        franks[rank] += 1\n",
    "\n",
    "    print()\n",
    "    top10 /= n\n",
    "    top100 /= n\n",
    "    mean_rank /= n\n",
    "    ftop10 /= n\n",
    "    ftop100 /= n\n",
    "    fmean_rank /= n\n",
    "\n",
    "    rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "    frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "    print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "    print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
